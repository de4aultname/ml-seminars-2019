\documentclass[pdf, 9pt, unicode]{beamer} %Для Latex2Pdf  tex ->%В качестве размера лучше 
\usepackage[T2A]{fontenc}
\usepackage[cp1251]{inputenc}
\usepackage[russian]{babel}
\usepackage{graphicx}
\usepackage{latexsym,amssymb,amsthm}
\usepackage{amsfonts}
\usepackage{amsmath}
\usefonttheme[onlymath]{serif}
\usepackage{beamerthemesplit}
\usetheme[numbers, totalnumbers, minimal]{Statmod}
\setbeamerfont{institute}{size=\normalsize}

\setbeamercolor{bluetext_color}{fg=blue}
\newcommand{\bluetext}[1]{{\usebeamercolor[fg]{bluetext_color}#1}}

\setbeamercolor{redtext_color}{fg=red}
\newcommand{\redtext}[1]{{\usebeamercolor[fg]{redtext_color}#1}}

\usepackage{wrapfig}

\usepackage{amsthm}
\usepackage{bm}
\usepackage{bbold}
\newtheorem{result}{Утверждение}
\newtheorem{remark}{Предложение}
\newtheorem{theorem+}{Теорема Гаусса-Маркова}
\newtheorem{theoremCT}{Теорема Куна-Таккера}
\DeclareMathOperator{\E}{E} 
\DeclareMathOperator{\T}{T} 
\DeclareMathOperator{\D}{D} 


\DeclareMathOperator{\w}{\mathsf{w}} 

\setbeamercovered{transparent}

\title[Обучение с учителем. SVM. Кросс-валидация.]{Обучение с учителем. Метод опорных векторов. Выбор модели с помощью кросс-валидации.}
\author[И. Лунев, М. Высоков, М. Петраков]{Лунев Иван, Высоков Максим, Петраков Михаил}
\institute[СПбГУ]{Санкт-Петербургский государственный университет \\
	Прикладная математика и информатика \\
	Кафедра статистического моделирования\\ 
}
\date{
	Санкт-Петербург\\
	2019г.
}
\begin{document}
	\begin{frame}
	\titlepage
\end{frame}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
\frametitle{Обучение с учителем. Постановка задачи}
$\mathsf{X}$ --- множество объектов, $\mathsf{Y}$ --- множество ответов \\
$\mathsf{y}: \mathsf{X}\to \mathsf{Y}$ --- неизвестная зависимость (target function) \\
~\\
Дано: обучающая выборка --- $\mathsf{(x_1,\ldots, x_n) \subset X}$, $\mathsf{y_i=y(x_i), ~i=1,\ldots,n}$ --- известные ответы. \\
Найти: $\mathsf{a:X\to Y}$ --- функцию (decision function), приближающую $\mathsf{y}$ на всем множестве $\mathsf{X}$. \\
~\\
Вероятностная постановка задачи: имеется неизвестное распределение на множестве $\mathsf{X\times Y}$ с плотностью $\mathsf{p(x,y)}$, из которого случайно выбираются $\mathbb{X}_\mathsf{n}=\mathsf{(x_i,y_i)_{i=1}^n}$ (независимые). \\
~\\
Задача классификации: 
\begin{itemize}
	\item $\mathsf{Y}=\{-1,+1\}$ --- классификация на 2 класса
	\item $\mathsf{Y}=\{1,\ldots,K\}$ --- классификация на $K$ классов
\end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
\frametitle{Задача построения разделяющей поверхности}


\begin{itemize}
	\item Задача классификации с двумя классами, $Y=\{-1,+1\}$: по обучающей выборке $X^n=(x_i,y_i)_{i=1}^n$ построить алгоритм классификации $a(x,\w)=\textrm{sign} f(x,\w)$,
	где $f(x,\w)$ --- разделяющая (дискриминантная) функция, $\w$ --- вектор параметров.
	
	\vspace{10mm}
	
	\item $f(x,\w)=0$ --- разделяющая поверхность;
	
	$M_i(\w)=y_if(x_i,\w)$ --- отступ (margin) объекта $x_i$;
	
	$M_i(\w)<0 \Leftrightarrow$  алгоритм $a(x,\w)$ ошибается на $x_i$.
	
	\vspace{10mm}
	
	\item Минимизация эмпирического риска:
	\begin{equation*}
		Q(\w)=\sum_{i=1}^{n}\textcolor{red}{[M_i(\w)<0]}\leq \tilde{Q}(\w)=\sum_{i=1}^{n}\textcolor{red}{\mathcal{L}(M_i(\w))}\to \min \limits_{\w};
	\end{equation*}
	
	функция потерь $\mathcal{L}(M_i(\w))$ невозрастающая, неотрицательная.

\end{itemize}


\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
\frametitle{Непрерывные аппроксимации пороговой функции потерь}
\begin{figure}
	\includegraphics[width=1\textwidth, height = 0.8\textheight]{pics/loss_functions.png}
\end{figure}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
\frametitle{Задача обучения линейного классификатора}

\textbf{Дано:}
\begin{itemize}
	\item Обучающая выборка $X^n = (x_i, y_i)_{i=1}^n$ 
	\item $x_i$ --- объекты, векторы из множества $X=\mathbb{R}^n$
	\item $y_i$ --- метки классов, элементы множества  $Y   =\{-1,1\}$
\end{itemize}


\textbf{Найти:}
Параметры $\w \in \mathbb{R}^p, \w_0 \in \mathbb{R}$ линейной модели классификации

\begin{equation*}
a(x;\w,\w_0)=\textrm{sign}(<x,\w>-\w_0).
\end{equation*}


\textbf{Критерий } --- минимизация эмпирического риска:

\begin{equation*}
\sum_{i=1}^{n}[a(x_i;\w,\w_0)\neq y_i]= \sum_{i=1}^{n}[M_i(\w,\w_0)<0] \to  \min \limits_{\w, \w_0},
\end{equation*}

где $M_i(\w,\w_0)=(<x,\w>-\w_0)y_i$ --- отступ (margin) объекта $x_i$.

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
\frametitle{Аппроксимация и регуляризация эмперического риска}

\begin{equation*}
Q(\w,\w_0)=\sum_{i=1}^{n}\bluetext{[M_i(\w,\w_0)<0]}\leq\sum_{i=1}^{n}\textcolor{red}{(1-M_i(\w,\w_0))_+}+\frac{1}{2C}||\w||^2 \to \min \limits_{\w,\w_0}
\end{equation*}


\vspace{10mm}

\begin{wrapfigure}{r}{0.5\textwidth} %this figure will be at the right
	\centering
	\includegraphics[width=0.4\textwidth]{pics/Emp_risk.png}
\end{wrapfigure}


\textbf{Аппроксимация} штрафует объекты за приближение к границе классов, увеличивая зазор между классами

\vspace{3mm}

\textbf{Регуляризация} штрафует неустойчивые решения в случае мультиколлинеарности


\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
\frametitle{Оптимальная разделяющая гиперплоскость}

\textbf{Линейный классификатор} : $a(x,\w)=\textrm{sign}(<x,\w>-\w_0)$

Пусть выборка $X^n=(x_i,y_i)_{i=1}^n линейна разделима:$
\begin{equation*}
\exists \w,\w_0: M_i(\w,\w_0)=(<x,\w>-\w_0)y_i>0, i=1\ldots n.
\end{equation*}


\vspace{3mm}

\textbf{Нормировка}: $\min \limits_{i=1,\ldots,n}M_i(\w,\w_0)=1$.

\vspace{3mm}

\begin{wrapfigure}{r}{0.5\textwidth} %this figure will be at the right
	\centering
	\includegraphics[width=0.4\textwidth]{pics/Opt_hyp.png}
\end{wrapfigure}

\textbf{Разделяющая полоса} (разделяющая гиперплоскость посередине):
$$
{x: -1 \leq <x,\w> - \w_0 \leq 1}
$$
$$
\exists x_{+}: <x_{+},\w> - \w_0 = 1
$$
$$
\exists x_{-}: <x_{-},\w> - \w_0 = -1
$$

\vspace{3mm}

\textbf{Ширина полосы:}
$$
\frac{<x_{+}-x_{-},\w>}{||\w||}=\frac{2}{||\w||}\to \max
$$

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
\frametitle{Обоснование кусочно-линейной функции потерь}

\textbf{Линейно разделимая выборка}: 

\begin{equation*}
\begin{cases}
\frac{1}{2} ||\w||^2 \to \min \limits_{\w, \w_0};\\
M_i(\w,\w_0)\geq1, & i=1,\ldots,n
\end{cases}
\end{equation*}


\vspace{3mm}

\textbf{Переход к линейно неразделимой выборки}: 

\begin{equation*}
\begin{cases}
\frac{1}{2} ||\w||^2 + \redtext{C\sum_{i=1}^{n} \xi_i }\to \min \limits_{\w, \w_0, \redtext{\xi}};\\
M_i(\w,\w_0)\geq1 \redtext{- \xi_i }, & i=1,\ldots,n;\\
\redtext{\xi_i \geq 0,  \quad i=1,\ldots,n.}
\end{cases}
\end{equation*}

\vspace{3mm}

\textbf{Эквивалентная задача безусловной минимизации}:

\begin{equation*}
C\sum_{i=1}^{n}(1-M_i(\w,\w_0))_++\frac{1}{2} ||\w||^2 \to \min \limits_{\w, \w_0, }.
\end{equation*}


\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{frame}
\frametitle{Влияние константы C на решение SVM}

SVM --- аппроксимация и регуляризация эмпирического риска:

$$
\sum_{i=1}^{n}(1-M_i(\w,\w_0))_++\frac{1}{2C}||\w||^2 \to \min \limits_{\w,\w_0}.
$$

\begin{figure}
	\includegraphics[width=1\textwidth, height = 0.5\textheight]{pics/SVM_C_param.png}
\end{figure}


\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
\frametitle{Напоминание. Условия Каруша-Куна-Таккера}

\textbf{Задача математического программирования}: 

\begin{equation*}
\begin{cases}
f(x) \to  \min \limits_{x}\\
g_i(x)\leq 0, \quad i=1,\ldots,m;\\
h_j(x)= 0, \quad i=1,\ldots,k;
\end{cases}
\end{equation*}

\vspace{3mm}

Необходимые условия. Если $x$ --- точка локального минимума, то существуют множители $\mu_i$, $i=1,\ldots,k$, $\lambda_j$, $j=1,\ldots,k$:

\vspace{3mm}

\begin{equation*}
\begin{cases}
\frac{\partial \mathcal{L}}{\partial x}=0, \quad \mathcal{L}(x;\mu,\lambda) = f(x)+\sum_{i=1}^{m}\mu_i g_i(x)+\sum_{j=1}^{k}\lambda_j h_j(x);\\
g_j(x)\leq 0; h_j(x)=0; \quad \text{(исходные ограничения)}\\
\mu_i \geq 0; \text{ (двойственные ограничения)}\\
\mu_ig_j(x)=0.\text{ (условия дополняющей нежесткости)}
\end{cases}
\end{equation*}


\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
\frametitle{Применение условий ККТ к задаче SVM}

\textbf{Функция Лагранжа}: 
$$
\mathcal{L}(\w,\w_0, \xi; \lambda, \mu) = \frac{1}{2} ||\w||^2  - \sum_{i=1}^{n}\lambda_i(M_i(\w,\w_0)-1)-\sum_{i=1}^{n}\xi_i(\lambda_i+\mu_i-C),
$$



\vspace{3mm}

$\lambda_i$ --- переменные, двойственные к ограничениям $M_i \geq 1 - \xi_i;$

$\mu_i$ --- переменные, двойственные к ограничениям $\xi_i \geq 0.$

\vspace{6mm}

\begin{equation*}
\begin{cases}
\frac{\partial \mathcal{L}}{\partial \w}=0, \frac{\partial \mathcal{L}}{\partial\w_0}=0, \frac{\partial \mathcal{L}}{\partial \xi}=0;\\
\xi_i\geq 0, \quad \lambda_i \geq 0, \quad \mu_i \geq 0, \quad i=1,\ldots,n;\\
\lambda_i = 0 \text{ либо } M_i(\w,\w_0)=1-\xi_i,\quad i=1,\ldots,n;\\
\mu_i = 0 \text{ либо } \xi_i=0,\quad i=1,\ldots,n.
\end{cases}
\end{equation*}


\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{frame}
\frametitle{Необходимые условия седловой точки Лагранжа}

\textbf{Функция Лагранжа}: 
$$
\mathcal{L}(\w,\w_0, \xi; \lambda, \mu) = \frac{1}{2} ||\w||^2  - \sum_{i=1}^{n}\lambda_i(M_i(\w,\w_0)-1)-\sum_{i=1}^{n}\xi_i(\lambda_i+\mu_i-C),
$$


\vspace{3mm}

\textbf{Необходимые условия седловой точки Лагранжа}: 

\begin{align*}
&\frac{\partial \mathcal{L}}{\partial \w}=\w-\sum_{i=1}^{n}\lambda_iy_ix_i=0 & \Longrightarrow & \quad\quad w=\sum_{i=1}^{n}\lambda_iy_ix_i;\\
&\frac{\partial \mathcal{L}}{\partial \w_0}=-\sum_{i=1}^{n}\lambda_iy_=0 & \Longrightarrow & \quad\quad \sum_{i=1}^{n}\lambda_iy_i=0;\\
&\frac{\partial \mathcal{L}}{\partial \xi_i}=-\lambda_i-\mu_i+C=0 & \Longrightarrow & \quad\quad \lambda_i+\mu_i=C, \quad i=1,\ldots,n.
\end{align*}


\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{frame}
\frametitle{Понятие опорного вектора}

\textbf{Типизация объектов}: 

\begin{itemize}
	\item $\alpha_i=0$; $\mu_i=C$; $\xi_i=0$; $ M_i\geq1$ --- периферийные (неинформативные) объекты;
	
	\item $0<\alpha_i<C$; $0<\mu_i<C$; $\xi_i=0$; $ M_i=1$ --- \redtext{опорные} граничные объекты;
	
	\item $\alpha_i=C$; $\mu_i=0$; $\xi_i>0$; $ M_i<1$ --- \redtext{опорные}-нарушители.
\end{itemize}

\vspace{9mm}

\begin{block}{Определение}
	Объект $x_i$ называется \redtext{опорным}, если $\lambda_i \neq 0$.
\end{block}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{frame}
\frametitle{Двойственная задача}

\begin{equation*}
\begin{cases}
	-\mathcal{L}(\lambda)=-\sum_{i=1}^{n}\lambda_i+\frac{1}{2}\sum_{i=1}^{n}\sum_{j=1}^{n}\lambda_i\lambda_jy_iy_j\redtext{<x_i,x_j>} \to \min \limits_{\lambda};\\
	0\leq\lambda_i\leq C \quad i=1,\ldots,n;\\
	\sum_{i=1}^{n}\lambda_iy_i=0.
\end{cases}
\end{equation*}

\vspace{3mm}

Решение прямой задачи выражается через решение двойственной:

\begin{equation*}
\begin{cases}
\w=\sum_{=1}^{n}\lambda_iy_ix_i;\\
\w_0=<\w_0,x_i>-y_i, \text{ для любого i: }\lambda_i>0, M_i=1.
\end{cases}
\end{equation*}

\vspace{3mm}

Линейный классификатор:
$$
a(x)=\textrm{sign}\left(\sum_{i=1}^{n}\lambda_iy_i\redtext{<x_i,x>}-\w_0\right).
$$

\end{frame}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
\frametitle{Нелинейное обобщение SVM}

Переход к спрямляющему пространству более высокой размерности: $\psi: X \to H$.

\begin{block}{Определение}
	Функция $K: X \times X \to \mathbb{R}$ --- ядро, если $K(x,x{'})=<\psi(x),\psi(x{'})>$ при некотором $\psi: X  \to H$, где $H$ --- гильбертово пространство.
\end{block}

\begin{block}{Теорема}
	Функция $K(x,x{'})$ является ядром тогда и только тогда, когда она симметрична: $K(x,x{'})=K(x{'},x)$; и неотрицательно определена:
	$$
	\int_{X}\int_XK(x,x{'})g(x)g(x{'})dxdx^{'} \geq 0 \text{ для людой }g: X \to \mathbb{R}.
	$$
\end{block}



\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
\frametitle{Конструктивные методы синтеза ядер}

\begin{enumerate}
	\item $K(x,x')=<x,x'>$ --- ядро; 
	
	\item константа $K(x,x')=1$ --- ядро;
	
	\item произведение ядер $K(x,x')=K_1(x,x')K_2(x,x')$ --- ядро;
	
	\item $\forall \psi X \to \mathbb{R}$ произведение $K(x,x')=\psi(x),\psi(x')$ --- ядро;
	
	\item $K(x,x')=\alpha_1K_1(x,x')+\alpha_2K_2(x,x')$ при $\alpha_1, \alpha_2 >0 $ --- ядро;
	
	\item $\forall \phi: X \to X$ если $K_0$ ядро, то $K(x,x')=K_0(\phi(x),\phi(x'))$ --- ядро;
	
	\item если $s: X \times X \to \mathbb{R}$ --- симметричная интегрируемая функция, то $K(x,x') = \int_{X}s(x,z)s(x',z)dz$ --- ядро;
	
	\item если $K_0$ --- ядро и функция $f: \mathbb{R} \to \mathbb{R}$ представима в виде сходящегося степенного ряда с неотрицательными коэффициентами, то $K(x,x')=f(K_0(x,x'))$ --- ядро.
\end{enumerate}



\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
\frametitle{Примеры ядер}

\begin{enumerate}
	\item $K(x,x')=<x,x'>^2$ --- квадратичное ядро; 
	
	\item $K(x,x')=<x,x'>^d$ --- полиномиальное ядро с мономами степени $d$; 
	
	\item $K(x,x')=(<x,x'>+1)^d$ --- полиномиальное ядро с мономами степени $\leq d$; 
	
	\item $K(x,x')=\sigma(<x,x'>)$ --- нейросеть с заданной функцией активации $\sigma(z)$ (не для всех $\sigma$  является ядром); 
	
	\item $K(x,x')=th(k_1<x,x'>-k_0), k_0,k_1\geq 0$ --- нейросеть с сигмоидными функциями активации; 
	
	\item $K(x,x')=exp(-\gamma ||x-x'||^2)$ ---сеть радиальных базисных функций (RBF ядро).
\end{enumerate}



\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
\frametitle{Классификация с различными ядрами}

Гиперплоскость в спрямляющем пространстве соответствует нелинейной разделяющей поверхности в исходном.
 
 \vspace{2mm}
 
Примеры с различными ядрами $K(x,x')$


\begin{figure}
	\includegraphics[width=1\textwidth, height = 0.5\textheight]{pics/Kernels.png}
\end{figure}


\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Двухслойная нейронная сеть}


\begin{figure}
	\includegraphics[width=1\textwidth, height = 0.8\textheight]{pics/NN.png}
\end{figure}


\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
\frametitle{SVM как двухслойная нейронная сеть}

Переномеруем объекты там, чтобы $x_1, \ldots, x_h$ были опорными.

$$
a(x)=\textrm{sign}\left(\sum_{i=1}^{h}\lambda_iy_iK(x,x_i)-\w_0
\right).
$$

\begin{figure}
	\includegraphics[width=1\textwidth, height = 0.5\textheight]{pics/SVM_NN.png}
\end{figure}

Первый слой вместо скалярных произведений вычисляет ядра.

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
\frametitle{Преимущества и недостатки SVM }

Преимущества SVM:
\begin{itemize}
	\item Задача выпуклого квадратичного программирования имеет единственное решение; 
	
	\item Число нейронов скрытого слоя определяется автоматически --- это число опорных векторов.
	
\end{itemize}

Недостатки SVM:
\begin{itemize}
	\item Неустойчивость к шуму; 
	
	\item Нет общий подходов к оптимизации $K(x,x')$ под задачу; 
	
	\item Приходится подбирать константу $C$; 
	
	\item Нет отбора признаков.
	
\end{itemize}


\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{frame}
\frametitle{Cross-validation}

Дано:
\begin{itemize}
	
	\item Имеется выборка (${X}_n$, ${Y}_n$);
	
	\item Умеем строить модель, зависящую от параметра $\theta$ и минимизирующую ошибку $J(X_n,Y_n; \theta, \lambda)$, где $\lambda$ --- параметр регуляризации; 
	
\end{itemize}

\vspace{3mm}

Хотим подобрать такой параметр $\theta_0$, чтобы минимизировать ошибку $J({X}_{new},{Y}_{new};\theta_0,0)$ на новых индивидах.

\vspace{3mm}

Алгоритм:
\begin{itemize}
	\item Делим выборку (${X}_n$, ${Y}_n$) случайным образом на три набора: (${X}_{train}$, ${Y}_{train}$), (${X}_{CV}$, ${Y}_{CV}$) и (${X}_{test}$, ${Y}_{test}$) ; 
	
	\item Перебираем набор параметров ${\lambda_1,\ldots,\lambda_m}$; 
	
	\item Для каждого параметра $\lambda_i$ строим модель на ($\mathsf{X}_{train}$, ${Y}_{train}$) (то есть находим оптимальное $\theta_{i0}$) и считаем ошибку на $J(X_{CV},Y_{CV};\theta_{i0}, 0)$; 
	
	\item Берем $\lambda_0$ c минимальной ошибкой (ему соответствует $\theta_0$);
	
	\item Считаем ошибку модели  $J(X_{test},Y_{test};\theta_{0},0)$.
	
\end{itemize}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{frame}
\frametitle{K-fold Cross-validation}


Условия такие же, как на предыдущем слайде.

\vspace{3mm}

Алгоритм:
\begin{itemize}
	\item Делим выборку (${X}_n$, ${Y}_n$) случайным образом на $K$ частей: (${X}_1$, ${Y}_1$),...,(${X}_K$, ${Y}_K$)  ; 
	
	\item Обозначим за (${X}_k'$, ${Y}_k'$) набор, содержащий всех индивидов, кроме (${X}_k$, ${Y}_k$);
	
	\item Перебираем набор параметров ${\lambda_1,\ldots,\lambda_m}$; 
	
	\item Для каждого параметра $\lambda_i$ считаем 
	$$
	CV_i=\sum_{j=1}^{K}\frac{n_j}{n} J({X}_j,{Y}_j;\theta_j,0),
	$$
	где $\theta_j$ минимизирует $J({X}_j',{Y}_j';\theta,\lambda_i)$, $n_j$ --- число индивидов в (${X}_j$, ${Y}_j$);
	
	\item Берем $\lambda_0$ c минимальной ошибкой $CV_i$;
	
	\item Берем $\theta_{0}$, которое минимизирует  $J(X_{n},Y_{n};\theta,\lambda_0)$.
	
\end{itemize}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}